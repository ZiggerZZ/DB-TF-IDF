{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Report for MAP543 - Database Management II  \n",
    "Authors: Roberta Conrad, Dora Ranilovic, Zigfrid Zvezdin  \n",
    "dependecies: Python 3.6.4+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF IDF Report\n",
    "\n",
    "## Table of Content \n",
    "\n",
    "1. Map Reduce Algorithm: a description of the _adopted solution_\n",
    "\n",
    "    1a. Other designed algorithms plus related global comments/description including comments to the code \n",
    "\n",
    "3. Experimental analysis, concerning in particular scalability \n",
    "\n",
    "    2a. Comments about the experimental analysis outlining weak and strong points of the algorithms.\n",
    "\n",
    "4. an appendix including all the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MapReduce algorithm\n",
    "calculate TF-IDF scores given an input set of documents. Provide both a MapReduce Python Hadoop streaming and a Spark implementation. \n",
    "## 1.a. Adopted Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Hadoop\n",
    "\n",
    "The first implementation of Tf-Idf is a tradidional MapReduce algorithm, designed to be run on a Hadoop cluster with only Python 2.7+ dependencies. It consists of consecutive Map and Reduce jobs, take a corpus of text documents as the input and compute the (nonzero) Tf-Idf scores for each word-document pair in the corpus. The following sections describe the code of all three jobs of the algorithm in detail, as well as the inputs and outputs. As this approach utilizes the traditional MapReduce framework, the inputs and outputs are always key-value pairs written in the format <(key, value)>. Aditionally, the output of each *reduce* step, is the raw input of the *map* step of the following job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job 1: Map\n",
    "\n",
    "__Input__: (doc, contents)\n",
    "\n",
    "__Output__: ((doc, term), 1)\n",
    "\n",
    "__Description__: Read text document line by line, envoking the source document name. Split each line into terms (words), convert to lowercase and remove punctuation, and emit the document name and term, together with the digit 1. This choice of output was made so that after the shuffle and sort step, the documents and identical words in each document will be grouped together to be able to perform two types of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip() #remove whitespace\n",
    "    terms = line.split(\" \") #create list of words\n",
    "    path = os.environ['mapreduce_map_input_file'].split(\"/\") #get full path of the text document\n",
    "    docname = path[-1]\n",
    "    for term in terms:\n",
    "        term = term.strip('''!()-[]{};:'\",<>./?@#$%^&*_~''').lower()\n",
    "        print('%s\\t%s' % (docname + '_' + term, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job 1: Reduce\n",
    "\n",
    "__Input__: ((doc, term), 1)\n",
    "\n",
    "__Output__: ((term, doc), (N, n))\n",
    "\n",
    "__Description__: This reduce step is computationally the most intensive as it performs two important types of counts at the same time: unique term per document (N) and term count (n). These two numbers together form the term freqency (Tf) part of the Tf-Idf expression. They are computed using nested loops and local lists, and taking advantage of the sorted nature of the inputs.\n",
    "The term count is accosiated to each (term, document) pair and is computed first, in the inside loop: the memory of the one previous term is kept and compared to the current term; if they match the counter is incremented an we move to the next term; once they no longer match, the term and its count are saved in a list, holding all terms in the current document. Once the end of the document is reached, all terms in the list are emitted one-by-one, along with the document name, their respective counts in the document and the length of the final list, which represents the number of unique terms in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "current_doc = None #Initialize helper variables\n",
    "current_term = None\n",
    "term_list = []\n",
    "current_term_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    pair, count = line.split('\\t', 1)\n",
    "    doc, term = pair.split('_', 1)\n",
    "\n",
    "    try: # convert count (currently a string) to int\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue  # count was not a number, so silently ignore/discard this line\n",
    "\n",
    "    if current_doc == doc: #outside loop, check if still in the same document\n",
    "        if current_term == term:\n",
    "            current_term_count += count\n",
    "        else:\n",
    "            term_list.append((current_term, current_term_count)) #hold term anf final count\n",
    "            current_term = term\n",
    "            current_term_count = count\n",
    "\n",
    "    else:\n",
    "        term_list.append((current_term, current_term_count)) #end of doc; emit all terms and counts\n",
    "        if current_doc:\n",
    "            for t, ct in term_list:\n",
    "                print('%s\\t%s' % (t+'_'+current_doc,\n",
    "                                  str(len(term_list))+'_'+(str(ct))))\n",
    "\n",
    "        current_term_count = count #reset counters\n",
    "        current_doc = doc\n",
    "        current_term = term\n",
    "        term_list = [] #reset list\n",
    "\n",
    "if term_list: #emit all terms except last in last doc\n",
    "    for t, ct in term_list:\n",
    "        print('%s\\t%s' % (t+'_'+current_doc,\n",
    "                          str(len(term_list)+1)+'_'+(str(ct))))\n",
    "\n",
    "print('%s\\t%s' % (current_term+'_'+current_doc, #emit last term in last doc\n",
    "                  str(len(term_list)+1)+'_'+str(current_term_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job 2: Map\n",
    "\n",
    "__Input__: ((term, doc), (N, n))\n",
    "\n",
    "__Output__: (term, (docname, N, n, 1))\n",
    "\n",
    "__Description__: The main purpose of this *map* step is to uncouple the key and pass only the term as the key to the next reducer, as we now want to calculate the document frequency of each term, and therefore need the terms grouped. Although we would achieve the same grouping by keeping the document name in the key (separated by an underscore, for example, from the term), we chose to perform the separation step in the mapper in order to take advantage of parallelization, and avoid performing the same task in the reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "for line in sys.stdin:\n",
    "    pair, vals = line.split('\\t')\n",
    "    term, docname = pair.split('_', 1)\n",
    "    N, n = vals.split('_')\n",
    "    try:\n",
    "        n = int(n)\n",
    "        N = int(N)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    print('%s\\t%s' % (term, docname+'_'+str(N)+'_'+str(n)+'_'+str(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job 2: Reduce\n",
    "\n",
    "__Input__: (term, (docname, N, n, 1))\n",
    "\n",
    "__Output__: ((term,docname), (N,n,d))\n",
    "\n",
    "__Description__: Much like a simple word count, count the occurence of each term, which thanks to the previous reducer will occur once per each document it appears in. Since we need the rest of the information found in the values of the key, we need to keep them in a local list until we reach the last occurence of the term, at which point we emit all term-document pairs for the current term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "current_term = None\n",
    "doc_list = []\n",
    "current_count = 0\n",
    "term = None\n",
    "doc = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    term, rest = line.split('\\t', 1)\n",
    "    doc, N, n, count = rest.rsplit('_', 3)\n",
    "    \n",
    "    try: # convert count (currently a string) to int\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently ignore/discard this line\n",
    "        continue\n",
    "        \n",
    "    if current_term == term:\n",
    "        doc_list.append((doc, N, n))\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_term:\n",
    "            for document, N, n in doc_list:\n",
    "                print(\n",
    "                    '%s\\t%s' % (current_term + '_' + document,\n",
    "                                str(N)+'_' + str(n) + '_' + str(current_count)))\n",
    "            doc_list = []\n",
    "            n_list = []\n",
    "        current_count = count\n",
    "        current_term = term\n",
    "        doc_list.append((doc, N, n))\n",
    "if current_term == term: #emit last term\n",
    "    for document, N, n in doc_list:\n",
    "        print('%s\\t%s' % (current_term + '_' + document,\n",
    "                          str(N)+'_' + str(n) + '_' + str(current_count)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job 3: Map\n",
    "\n",
    "__Input__: ((term, docname), (N, n, d))\n",
    "\n",
    "__Output__: ((term, docname), tfidf)\n",
    "\n",
    "__Description__: Having all necessary elements, compute the final Tf-Idf score for each (term, document) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "\n",
    "for line in sys.stdin:\n",
    "\n",
    "    pair, vals = line.split('\\t', 1)\n",
    "    N, n, d = vals.split('_', 2)\n",
    "    try:\n",
    "        N = int(N)\n",
    "        n = int(n)\n",
    "        d = int(d)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    tf = n/N\n",
    "    idf = math.log(10000/(1+d)) # use 10,000 as max number of documents\n",
    "    tfidf = tf * idf\n",
    "    print('%s\\t%s' % (pair, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job 3: Reduce\n",
    "\n",
    "__Input__: ((term, docname), tfidf)\n",
    "\n",
    "__Output__: ((term, docname), tfidf)\n",
    "\n",
    "__Description__: Analogous to a pass task, simply emit the input without any additional tratment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark\n",
    "\n",
    "## 1.b. Other designed algorithms\n",
    "\n",
    "### Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experimental analysis\n",
    "\n",
    "In order to compare the performance of the three different implementations, we designed two different tests considering different structures of a text collection with increasing sizes. We differentiated between a collection of documents containing many short documents; and a collection containing few long documents:\n",
    "\n",
    "+ Collections with a fixed document size: we fixed the number of words at 100 words per document, changing the number of documents in (100,200,300,400,500,1000).  \n",
    "+ Collections with a fixed number of documents: We ran our tf-idf algorithms on 100 documents containing (100,200,300,400,500,1000) words. \n",
    "\n",
    "For easy reproducability this report contains all code and scripts that have been used to produce the following analysis and can be recreated on AWS Elastic Map Reduce cluster. \n",
    "\n",
    "*Limitations*  \n",
    "In order to control the size of the documents used to test our algorithm, we decided to write a script creating documents for testing purposes. One document containing 100 words has a size of 11 KB when created by our script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Comparison of the implementations\n",
    "\n",
    "The second Spark implementation outperforms the other algorithms for almost all input sizes and scales as document size and/or number increase. We prefer the second Spark algorithm for both a few, long documents, as well as many very short ones - such as tweets or emails, for example. \n",
    "\n",
    "The first Spark algorithm is the most efficient algorithm for a very small text collection. However, it does not scale with size and turns out to be very slow for increasingly larger document sizes, while it shows the exact same performance as the fist Spark implementation for many small documents. \n",
    "\n",
    "The MapReduce implementation is the slowest option of all three and only better then the first Spark implementation as documents become longer then 300 Words (approx. 30 KB). \n",
    "\n",
    "<img src=\"./longdoc.PNG\">\n",
    "\n",
    "<img src=\"./Manydoc.PNG\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Practical Instructions to run tests on the cluster\n",
    "\n",
    "1. Create text documents on hadoop home directory  \n",
    "    + create the input directory on /user/hadoop local file system \n",
    "    + in /input, import the python script and execute the text_generator function to create documents needed, specify params as described below. \n",
    "    + move only the .txt to the hdfs hadoop input directory "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python -m nltk.downloader brown\n",
    "\n",
    "mkdir input\n",
    "cd input\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/text_docs/text_generator.py\n",
    "\n",
    "python -c 'from text_generator import create_docs; create_docs(number_of_words_per_doc = 200, num_doc = 10,startnr = 23)'\n",
    "\n",
    "cd\n",
    "\n",
    "hdfs dfs -mkdir /user/hadoop/tfidf/input\n",
    "hdfs dfs -put input/*.txt /user/hadoop/tfidf/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Download and run the MapReduce algorithms\n",
    " + give permission to access all documents\n",
    " + execute all jobs through bash script runjobs.sh\n",
    " + after retrieving the time information, clear input directories with clear.sh"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/mapper1.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/mapper2.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/mapper3.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/reducer1.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/reducer2.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/reducer3.py\n",
    "\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/runjobs.sh\n",
    "\n",
    "chmod +x *.py\n",
    "chmod +x *.sh\n",
    "\n",
    "time sh runjobs.sh\n",
    "\n",
    "sh clear.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Code Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_generator.py\n",
    "*Requirements* Requires installing the nlkt library brown.   \n",
    "\n",
    "*Functionality* The function create_docs takes as input parameters the number of words per document, number of documents, and start number of the document to be taken into account when naming files. The function creates the specified number of documents at the specified length by taking samples of the brown corpus. It randomly samples lines of twenty words at the same time.\n",
    "\n",
    "The Brown University Standard Corpus of Present-Day American English (or just Brown Corpus) was compiled in the 1960s as a general corpus. It contains 500 samples of English-language text, totaling roughly one million words, compiled from works published in the United States in 1961. It is a suitable library to sample from in order to test our algorithm as it represents how language is used in reality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "from nltk.corpus import brown\n",
    "import random\n",
    "corpus_length = len(brown.words())\n",
    "hardcopy = brown.words()\n",
    "\n",
    "\n",
    "def create_docs(number_of_words_per_doc=200, num_doc=10, startnr=0):\n",
    "    # control number of words per doc\n",
    "    # and number of documents\n",
    "    # we fix the line length at 20\n",
    "    line_length = 20\n",
    "    number_of_lines = int(number_of_words_per_doc / line_length)\n",
    "\n",
    "    for i in range(0, num_doc):\n",
    "        # create new file with writing + permission\n",
    "        new_file = open(\"textdoc\"+str(number_of_words_per_doc) +\n",
    "                        \"words\" + str(i+startnr)+\".txt\", \"w+\")\n",
    "        for line in range(0, number_of_lines):\n",
    "            words = list(map(\n",
    "                lambda x: hardcopy[x:x+line_length], random.sample(range(corpus_length), line_length)))\n",
    "            sentences = list(\n",
    "                map(lambda x: ' '.join(word for word in x), words))\n",
    "            text = ''.join(map(str, sentences))\n",
    "            new_file.write(text + \"\\n\")\n",
    "        new_file.close()\n",
    "        if (i % 10 == 0):\n",
    "            print(\"You created \"+str(i)+\" files! \"+str(num_doc-i)+\" left\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
