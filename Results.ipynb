{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Report for MAP543 - Database Management II  \n",
    "Authors: Roberta Conrad, Dora Ranilovic, Zigfrid Zvezdin  \n",
    "dependecies: Python 3.6.4+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF IDF Report\n",
    "\n",
    "## Table of Content \n",
    "\n",
    "1. Map Reduce Algorithm: a description of the _adopted solution_\n",
    "\n",
    "    1a. Other designed algorithms plus related global comments/description including comments to the code \n",
    "\n",
    "3. Experimental analysis, concerning in particular scalability \n",
    "\n",
    "    2a. Comments about the experimental analysis outlining weak and strong points of the algorithms.\n",
    "\n",
    "4. an appendix including all the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MapReduce algorithm\n",
    "calculate TF-IDF scores given an input set of documents. Provide both a MapReduce Python Hadoop streaming and a Spark implementation. \n",
    "## 1.a. Adopted Version\n",
    "\n",
    "### Python Hadoop\n",
    "\n",
    "The first implementation of Tf-Idf is a tradidional MapReduce algorithm, designed to be run on a Hadoop cluster with only Python 2.7/3.+ dependencies. It consists of consecutive Map and Reduce jobs, with the following inputs and outputs:\n",
    "\n",
    "### Spark\n",
    "\n",
    "## 1.b. Other designed algorithms\n",
    "\n",
    "### Python Hadoop\n",
    "\n",
    "### Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experimental analysis\n",
    "\n",
    "In order to compare the performance of the three different implementations, we designed two different tests considering different structures of a text collection with increasing sizes. We differentiated between a collection of documents containing many short documents; and a collection containing few long documents:\n",
    "\n",
    "+ Collections with a fixed document size: we fixed the number of words at 100 words per document, changing the number of documents in (100,200,300,400,500,1000).  \n",
    "+ Collections with a fixed number of documents: We ran our tf-idf algorithms on 100 documents containing (100,200,300,400,500,1000) words. \n",
    "\n",
    "For easy reproducability this report contains all code and scripts that have been used to produce the following analysis and can be recreated on AWS Elastic Map Reduce cluster. \n",
    "\n",
    "*Limitations*  \n",
    "In order to control the size of the documents used to test our algorithm, we decided to write a script creating documents for testing purposes. One document containing 100 words has a size of 11 KB when created by our script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Comparison of the implementations\n",
    "\n",
    "The second Spark implementation outperforms the other algorithms for almost all input sizes and scales as document size and/or number increase. We prefer the second Spark algorithm for both a few, long documents, as well as many very short ones - such as tweets or emails, for example. \n",
    "\n",
    "The first Spark algorithm is the most efficient algorithm for a very small text collection. However, it does not scale with size and turns out to be very slow for increasingly larger document sizes, while it shows the exact same performance as the fist Spark implementation for many small documents. \n",
    "\n",
    "The MapReduce implementation is the slowest option of all three and only better then the first Spark implementation as documents become longer then 300 Words (approx. 30 KB). \n",
    "\n",
    "<img src=\"./longdoc.PNG\">\n",
    "\n",
    "<img src=\"./Manydoc.PNG\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Practical Instructions to run tests on the cluster\n",
    "\n",
    "1. Create text documents on hadoop home directory  \n",
    "    + create the input directory on /user/hadoop local file system \n",
    "    + in /input, import the python script and execute the text_generator function to create documents needed, specify params as described below. \n",
    "    + move only the .txt to the hdfs hadoop input directory "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python -m nltk.downloader brown\n",
    "\n",
    "mkdir input\n",
    "cd input\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/text_docs/text_generator.py\n",
    "\n",
    "python -c 'from text_generator import create_docs; create_docs(number_of_words_per_doc = 200, num_doc = 10,startnr = 23)'\n",
    "\n",
    "cd\n",
    "\n",
    "hdfs dfs -mkdir /user/hadoop/tfidf/input\n",
    "hdfs dfs -put input/*.txt /user/hadoop/tfidf/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Download and run the MapReduce algorithms\n",
    " + give permission to access all documents\n",
    " + execute all jobs through bash script runjobs.sh\n",
    " + after retrieving the time information, clear input directories with clear.sh"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/mapper1.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/mapper2.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/mapper3.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/reducer1.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/reducer2.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/reducer3.py\n",
    "\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/runjobs.sh\n",
    "\n",
    "chmod +x *.py\n",
    "chmod +x *.sh\n",
    "\n",
    "time sh runjobs.sh\n",
    "\n",
    "sh clear.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Code Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_generator.py\n",
    "*Requirements* Requires installing the nlkt library brown.   \n",
    "\n",
    "*Functionality* The function create_docs takes as input parameters the number of words per document, number of documents, and start number of the document to be taken into account when naming files. The function creates the specified number of documents at the specified length by taking samples of the brown corpus. It randomly samples lines of twenty words at the same time.\n",
    "\n",
    "The Brown University Standard Corpus of Present-Day American English (or just Brown Corpus) was compiled in the 1960s as a general corpus. It contains 500 samples of English-language text, totaling roughly one million words, compiled from works published in the United States in 1961. It is a suitable library to sample from in order to test our algorithm as it represents how language is used in reality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "from nltk.corpus import brown\n",
    "import random\n",
    "corpus_length = len(brown.words())\n",
    "hardcopy = brown.words()\n",
    "\n",
    "\n",
    "def create_docs(number_of_words_per_doc=200, num_doc=10, startnr=0):\n",
    "    # control number of words per doc\n",
    "    # and number of documents\n",
    "    # we fix the line length at 20\n",
    "    line_length = 20\n",
    "    number_of_lines = int(number_of_words_per_doc / line_length)\n",
    "\n",
    "    for i in range(0, num_doc):\n",
    "        # create new file with writing + permission\n",
    "        new_file = open(\"textdoc\"+str(number_of_words_per_doc) +\n",
    "                        \"words\" + str(i+startnr)+\".txt\", \"w+\")\n",
    "        for line in range(0, number_of_lines):\n",
    "            words = list(map(\n",
    "                lambda x: hardcopy[x:x+line_length], random.sample(range(corpus_length), line_length)))\n",
    "            sentences = list(\n",
    "                map(lambda x: ' '.join(word for word in x), words))\n",
    "            text = ''.join(map(str, sentences))\n",
    "            new_file.write(text + \"\\n\")\n",
    "        new_file.close()\n",
    "        if (i % 10 == 0):\n",
    "            print(\"You created \"+str(i)+\" files! \"+str(num_doc-i)+\" left\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
