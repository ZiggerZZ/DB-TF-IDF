{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Roberta, Dora, Zigfrid  \n",
    "dependecies: Python 3.6.4+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF IDF Report\n",
    "\n",
    "## Table of Content \n",
    "\n",
    "1. Map Reduce Algorithm: a description of the _adopted solution_\n",
    "\n",
    "    1a. Other designed algorithms plus related global comments/description including comments to the code \n",
    "\n",
    "3. Experimental analysis, concerning in particular scalability \n",
    "\n",
    "    2a. Comments about the experimental analysis outlining weak and strong points of the algorithms.\n",
    "\n",
    "4. an appendix including all the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MapReduce algorithm\n",
    "calculate TF-IDF scores given an input set of documents. Provide both a MapReduce Python Hadoop streaming and a Spark implementation. \n",
    "## 1.a. Adopted Version\n",
    "\n",
    "### Python Hadoop\n",
    "\n",
    "### Spark\n",
    "\n",
    "## 1.b. Other designed algorithms\n",
    "\n",
    "### Python Hadoop\n",
    "\n",
    "### Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experimental analysis\n",
    "compare performances of the two implementation. To test scalability consider 5 document sets of increasing sizes. For instance, size can double from a set to another including  comments about the experimental analysis outlining weak and strong points of the algorithms.\n",
    "\n",
    "Text source ideas - project guttenberg: https://www.gutenberg.org/\n",
    "movie transcripts - https://www.imsdb.com/TV/Futurama.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Instructions to run tests on the cluster\n",
    "\n",
    "Collect speed information here: https://docs.google.com/spreadsheets/d/1woubFQECRr-0GMBUtKGj6vq0i8aybqAdAYiR-eaTaOY/edit?usp=sharing\n",
    "\n",
    "1. Create text\n",
    "* create input directory on /user/hadoop local file system \n",
    "* in /input, wget python script and execute the text_generator function to create documents needed, specify params as described below. if you already created soem files, remember to change the startnr.\n",
    "* afterwards, move only the .txt to the hdfs hadoop input directory "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python -m nltk.downloader brown\n",
    "\n",
    "mkdir input\n",
    "cd input\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/text_docs/text_generator.py\n",
    "\n",
    "python -c 'from text_generator import create_docs; create_docs(number_of_words_per_doc = 200, num_doc = 10,startnr = 23)'\n",
    "\n",
    "cd\n",
    "\n",
    "hdfs dfs -mkdir /user/hadoop/tfidf\n",
    "hdfs dfs -put input/*.txt /user/hadoop/tfidf/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Download the scripts mapper 1 - reducer 3, and the .bash script\n",
    " + give permission "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/mapper1.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/mapper2.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/mapper3.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/reducer1.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/reducer2.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/reducer3.py\n",
    "\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/runjobs.sh\n",
    "\n",
    "chmod +x *.py\n",
    "chmod +x *.sh\n",
    "\n",
    "time sh runjobs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After one job is done, remove all input and output directories, and create a new input directory on hdfs to be filled with the new text documents."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hdfs dfs -rm -r /user/hadoop/tfidf/input\n",
    "hdfs dfs -rm -r /user/hadoop/tfidf/output\n",
    "hdfs dfs -rm -r /user/hadoop/tfidf/output1\n",
    "hdfs dfs -rm -r /user/hadoop/tfidf/output2\n",
    "hdfs dfs -mkdir /user/hadoop/tfidf/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Code Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
