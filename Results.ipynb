{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Roberta, Dora, Zigfrid  \n",
    "dependecies: Python 3.6.4+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF IDF Report\n",
    "\n",
    "## Table of Content \n",
    "\n",
    "1. Map Reduce Algorithm: a description of the _adopted solution_\n",
    "\n",
    "    1a. Other designed algorithms plus related global comments/description including comments to the code \n",
    "\n",
    "3. Experimental analysis, concerning in particular scalability \n",
    "\n",
    "    2a. Comments about the experimental analysis outlining weak and strong points of the algorithms.\n",
    "\n",
    "4. an appendix including all the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MapReduce algorithm\n",
    "calculate TF-IDF scores given an input set of documents. Provide both a MapReduce Python Hadoop streaming and a Spark implementation. \n",
    "## 1.a. Adopted Version\n",
    "\n",
    "### Python Hadoop\n",
    "\n",
    "The first implementation of Tf-Idf is a tradidional MapReduce algorithm, designed to be run on a Hadoop cluster with only Python 2.7/3.+ dependencies. It consists of consecutive Map and Reduce jobs, with the following inputs and outputs:\n",
    "\n",
    "### Spark\n",
    "\n",
    "## 1.b. Other designed algorithms\n",
    "\n",
    "### Python Hadoop\n",
    "\n",
    "### Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experimental analysis\n",
    "compare performances of the two implementation. To test scalability consider 5 document sets of increasing sizes. For instance, size can double from a set to another including  comments about the experimental analysis outlining weak and strong points of the algorithms.\n",
    "\n",
    "Text source ideas - project guttenberg: https://www.gutenberg.org/\n",
    "movie transcripts - https://www.imsdb.com/TV/Futurama.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Instructions to run tests on the cluster\n",
    "\n",
    "Collect speed information here: https://docs.google.com/spreadsheets/d/1woubFQECRr-0GMBUtKGj6vq0i8aybqAdAYiR-eaTaOY/edit?usp=sharing\n",
    "\n",
    "1. Create text\n",
    "* create input directory on /user/hadoop local file system \n",
    "* in /input, wget python script and execute the text_generator function to create documents needed, specify params as described below. if you already created soem files, remember to change the startnr.\n",
    "* afterwards, move only the .txt to the hdfs hadoop input directory "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python -m nltk.downloader brown\n",
    "\n",
    "mkdir input\n",
    "cd input\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/text_docs/text_generator.py\n",
    "\n",
    "python -c 'from text_generator import create_docs; create_docs(number_of_words_per_doc = 200, num_doc = 10,startnr = 23)'\n",
    "\n",
    "cd\n",
    "\n",
    "hdfs dfs -mkdir /user/hadoop/tfidf/input\n",
    "hdfs dfs -put input/*.txt /user/hadoop/tfidf/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Download the scripts mapper 1 - reducer 3, and the .bash script\n",
    " + give permission to access all documents\n",
    " + execute all jobs through bash script runjobs.sh"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/mapper1.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/mapper2.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/mapper3.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/reducer1.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/reducer2.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/reducer3.py\n",
    "\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/runjobs.sh\n",
    "\n",
    "chmod +x *.py\n",
    "chmod +x *.sh\n",
    "\n",
    "time sh runjobs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After one job is done, remove all input and output directories, and create a new input directory on hdfs to be filled with the new text documents."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hdfs dfs -rm -r /user/hadoop/tfidf/input\n",
    "hdfs dfs -rm -r /user/hadoop/tfidf/output\n",
    "hdfs dfs -rm -r /user/hadoop/tfidf/output1\n",
    "hdfs dfs -rm -r /user/hadoop/tfidf/output2\n",
    "hdfs dfs -mkdir /user/hadoop/tfidf/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Code Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text_generator.py\n",
    "*Requirements* Requires installing the nlkt library brown (python -m nltk.downloader brown). \n",
    "\n",
    "\n",
    "*Functionality* The function create_docs takes as input parameters the number of words per document, number of documents, and start number of the document to be taken into account when naming files. The function creates the specified number of documents at the specified length by taking samples of the brown corpus. The Brown University Standard Corpus of Present-Day American English (or just Brown Corpus) was compiled in the 1960s as a general corpus. It contains 500 samples of English-language text, totaling roughly one million words, compiled from works published in the United States in 1961. It is a suitable library to sample from in order to test our algorithm as it represents how language is used in reality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "from nltk.corpus import brown\n",
    "import random\n",
    "corpus_length = len(brown.words())\n",
    "hardcopy = brown.words()\n",
    "\n",
    "\n",
    "def create_docs(number_of_words_per_doc=200, num_doc=10, startnr=0):\n",
    "    # control number of words per doc\n",
    "    # and number of documents\n",
    "    # we fix the line length at 20\n",
    "    line_length = 20\n",
    "    number_of_lines = int(number_of_words_per_doc / line_length)\n",
    "\n",
    "    for i in range(0, num_doc):\n",
    "        # create new file with writing + permission\n",
    "        new_file = open(\"textdoc\"+str(number_of_words_per_doc) +\n",
    "                        \"words\" + str(i+startnr)+\".txt\", \"w+\")\n",
    "        for line in range(0, number_of_lines):\n",
    "            words = list(map(\n",
    "                lambda x: hardcopy[x:x+line_length], random.sample(range(corpus_length), line_length)))\n",
    "            sentences = list(\n",
    "                map(lambda x: ' '.join(word for word in x), words))\n",
    "            text = ''.join(map(str, sentences))\n",
    "            new_file.write(text + \"\\n\")\n",
    "        new_file.close()\n",
    "        if (i % 10 == 0):\n",
    "            print(\"You created \"+str(i)+\" files! \"+str(num_doc-i)+\" left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
