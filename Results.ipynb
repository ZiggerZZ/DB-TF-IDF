{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report for MAP543 - Database Management II  \n",
    "Authors: Roberta Conrad, Dora Ranilovic, Zigfrid Zvezdin  \n",
    "Dependencies: Python 3.6.4+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF IDF Report\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Tf-Idf Algorithm: a description of the _adopted solution_\n",
    "\n",
    "    1a. Other designed algorithms plus related global comments/description including comments to the code\n",
    "    \n",
    "2. Experimental analysis, concerning in particular scalability \n",
    "\n",
    "    2a. Comments about the experimental analysis outlining weak and strong points of the algorithms.\n",
    "    \n",
    "3. Appendix including all the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tf-Idf Algorithm\n",
    "\n",
    "Tf-Idf is a widely used statistic in the field of information retreival, used to quantify the relevance of a query or key word to a document inside a corpus.  It is commonly used in search engine optimization and text mining. The statistic is designed to assigns higher relevance to a term if it occurs often in the document, but penalize it if it occus in many different documents, i.e. it is not unique or specific to one or few documents. The general formula to compute the Tf-Idf score for term _t_ and documet _d_ in corpus _D_ is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "$tf.idf(t,d,D)=tf(t,d)*idf(t,D)$ <br><br>\n",
    "$=\\frac{n(t,d)}{N(d)}*log\\frac{|D|}{d(t)}$<br><br>\n",
    "</center>\n",
    "where:<br><br>\n",
    "$tf(t,d)$ is the term frequency function<br><br>\n",
    "$idf(t,D)$ is the inverse document frequency function<br><br>\n",
    "$n(t,d)$ is the number of times t occurs in d<br><br>\n",
    "$N(d)$ is the number of unique terms in d<br><br>\n",
    "$d(t)$ is the number of documents in D which contain t<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a. Baseline Algorithms\n",
    "\n",
    "This section describes in detail the two baseline implementations of the Tf-Idf algorithm we developed and tested: a Python/Hadoop algorithm, and a Spark algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Hadoop\n",
    "\n",
    "The first implementation of Tf-Idf is a traditional MapReduce algorithm, designed to be run on a Hadoop cluster with only Python 2.7+ dependencies. It consists of consecutive Map and Reduce jobs, take a corpus of text documents as the input and compute the (nonzero) Tf-Idf scores for each word-document pair in the corpus. The following sections discuss the code of all three jobs, as well as the inputs and outputs. As this approach utilizes the traditional MapReduce framework, the inputs and outputs are always key-value pairs written in the format <(key, value)>. Aditionally, the output of each *reduce* step, is the raw input of the *map* step of the following job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job 1: Map\n",
    "\n",
    "__Input__: (doc, contents)\n",
    "\n",
    "__Output__: ((doc, term), 1)\n",
    "\n",
    "__Description__: Read text document line by line, envoking the source document name. Split each line into terms (words), convert to lowercase and remove punctuation, and emit the document name and term, together with the digit 1. This choice of output was made so that after the shuffle and sort step, the documents and identical words in each document will be grouped together to be able to perform two types of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip() #remove whitespace\n",
    "    terms = line.split(\" \") #create list of words\n",
    "    path = os.environ['mapreduce_map_input_file'].split(\"/\") #get full path of the text document\n",
    "    docname = path[-1]\n",
    "    for term in terms:\n",
    "        term = term.strip('''!()-[]{};:'\",<>./?@#$%^&*_~''').lower()\n",
    "        print('%s\\t%s' % (docname + '_' + term, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job 1: Reduce\n",
    "\n",
    "__Input__: ((doc, term), 1)\n",
    "\n",
    "__Output__: ((term, doc), (N, n))\n",
    "\n",
    "__Description__: This reduce step is computationally the most intensive as it performs two important types of counts at the same time: unique term per document (N) and term count (n). These two numbers together form the term freqency (Tf) part of the Tf-Idf expression. They are computed using nested loops and local lists, and taking advantage of the sorted nature of the inputs.\n",
    "The term count is accosiated to each (term, document) pair and is computed first, in the inside loop: the memory of the one previous term is kept and compared to the current term; if they match the counter is incremented an we move to the next term; once they no longer match, the term and its count are saved in a list, holding all terms in the current document. Once the end of the document is reached, all terms in the list are emitted one-by-one, along with the document name, their respective counts in the document and the length of the final list, which represents the number of unique terms in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "current_doc = None #Initialize helper variables\n",
    "current_term = None\n",
    "term_list = []\n",
    "current_term_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    pair, count = line.split('\\t', 1)\n",
    "    doc, term = pair.split('_', 1)\n",
    "\n",
    "    try: # convert count (currently a string) to int\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue  # count was not a number, so silently ignore/discard this line\n",
    "\n",
    "    if current_doc == doc: #outside loop, check if still in the same document\n",
    "        if current_term == term:\n",
    "            current_term_count += count\n",
    "        else:\n",
    "            term_list.append((current_term, current_term_count)) #hold term anf final count\n",
    "            current_term = term\n",
    "            current_term_count = count\n",
    "\n",
    "    else:\n",
    "        term_list.append((current_term, current_term_count)) #end of doc; emit all terms and counts\n",
    "        if current_doc:\n",
    "            for t, ct in term_list:\n",
    "                print('%s\\t%s' % (t+'_'+current_doc,\n",
    "                                  str(len(term_list))+'_'+(str(ct))))\n",
    "\n",
    "        current_term_count = count #reset counters\n",
    "        current_doc = doc\n",
    "        current_term = term\n",
    "        term_list = [] #reset list\n",
    "\n",
    "if term_list: #emit all terms except last in last doc\n",
    "    for t, ct in term_list:\n",
    "        print('%s\\t%s' % (t+'_'+current_doc,\n",
    "                          str(len(term_list)+1)+'_'+(str(ct))))\n",
    "\n",
    "print('%s\\t%s' % (current_term+'_'+current_doc, #emit last term in last doc\n",
    "                  str(len(term_list)+1)+'_'+str(current_term_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job 2: Map\n",
    "\n",
    "__Input__: ((term, doc), (N, n))\n",
    "\n",
    "__Output__: (term, (docname, N, n, 1))\n",
    "\n",
    "__Description__: The main purpose of this *map* step is to uncouple the key and pass only the term as the key to the next reducer, as we now want to calculate the document frequency of each term, and therefore need the terms grouped. Although we would achieve the same grouping by keeping the document name in the key (separated by an underscore, for example, from the term), we chose to perform the separation step in the mapper in order to take advantage of parallelization, and avoid performing the same task in the reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "for line in sys.stdin:\n",
    "    pair, vals = line.split('\\t')\n",
    "    term, docname = pair.split('_', 1)\n",
    "    N, n = vals.split('_')\n",
    "    try:\n",
    "        n = int(n)\n",
    "        N = int(N)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    print('%s\\t%s' % (term, docname+'_'+str(N)+'_'+str(n)+'_'+str(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job 2: Reduce\n",
    "\n",
    "__Input__: (term, (docname, N, n, 1))\n",
    "\n",
    "__Output__: ((term,docname), (N,n,d))\n",
    "\n",
    "__Description__: Much like a simple word count, count the occurence of each term, which thanks to the previous reducer will occur once per each document it appears in. Since we need the rest of the information found in the values of the key, we need to keep them in a local list until we reach the last occurence of the term, at which point we emit all term-document pairs for the current term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "current_term = None\n",
    "doc_list = []\n",
    "current_count = 0\n",
    "term = None\n",
    "doc = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    term, rest = line.split('\\t', 1)\n",
    "    doc, N, n, count = rest.rsplit('_', 3)\n",
    "    \n",
    "    try: # convert count (currently a string) to int\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently ignore/discard this line\n",
    "        continue\n",
    "        \n",
    "    if current_term == term:\n",
    "        doc_list.append((doc, N, n))\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_term:\n",
    "            for document, N, n in doc_list:\n",
    "                print(\n",
    "                    '%s\\t%s' % (current_term + '_' + document,\n",
    "                                str(N)+'_' + str(n) + '_' + str(current_count)))\n",
    "            doc_list = []\n",
    "            n_list = []\n",
    "        current_count = count\n",
    "        current_term = term\n",
    "        doc_list.append((doc, N, n))\n",
    "if current_term == term: #emit last term\n",
    "    for document, N, n in doc_list:\n",
    "        print('%s\\t%s' % (current_term + '_' + document,\n",
    "                          str(N)+'_' + str(n) + '_' + str(current_count)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job 3: Map\n",
    "\n",
    "__Input__: ((term, docname), (N, n, d))\n",
    "\n",
    "__Output__: ((term, docname), tfidf)\n",
    "\n",
    "__Description__: Having all necessary elements, compute the final Tf-Idf score for each (term, document) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "\n",
    "for line in sys.stdin:\n",
    "\n",
    "    pair, vals = line.split('\\t', 1)\n",
    "    N, n, d = vals.split('_', 2)\n",
    "    try:\n",
    "        N = int(N)\n",
    "        n = int(n)\n",
    "        d = int(d)\n",
    "    except ValueError:\n",
    "        continue\n",
    "    tf = n/N\n",
    "    idf = math.log(10000/(1+d)) # use 10,000 as max number of documents\n",
    "    tfidf = tf * idf\n",
    "    print('%s\\t%s' % (pair, tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Job 3: Reduce\n",
    "\n",
    "__Input__: ((term, docname), tfidf)\n",
    "\n",
    "__Output__: ((term, docname), tfidf)\n",
    "\n",
    "__Description__: Analogous to a pass task, simply emit the input without any additional tratment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark\n",
    "\n",
    "## 1.b. Other Designed Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark 1\n",
    "This algorithm is good for big collection of small documents as it parallelizes the documents over machine with funciton wholeTextFiles()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def word_freq(doc):\n",
    "    \"\"\"Function to compute frequencies of occurences of words in a document.\n",
    "\n",
    "    Args:\n",
    "        doc: Document = string with lines separated by '\\n'.\n",
    "\n",
    "    Returns:\n",
    "        List of pairs (word, frequency) corresponding to the doc.\n",
    "    \"\"\"\n",
    "    lines = doc.lower().split('\\n')\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        for word in line.split():\n",
    "            clean_word = ''.join(x for x in word if x.isalpha())\n",
    "            words.append(clean_word)\n",
    "    occurrences = []\n",
    "    for word in words:\n",
    "        occurrences.append(words.count(word))\n",
    "    N = len(words)\n",
    "    freqs = [w/N for w in occurrences]\n",
    "    wordfreq = set(zip(words, freqs))\n",
    "    return wordfreq\n",
    "\n",
    "def word_to_key(x):\n",
    "    \"\"\"Function to compute frequencies of occurences of words in a document.\n",
    "\n",
    "    Args:\n",
    "        x: Pair (document_name,(word,frequency)).\n",
    "\n",
    "    Returns:\n",
    "        List of pairs (word,(document_name,frequency)).\n",
    "    \"\"\"\n",
    "    file = x[0]\n",
    "    l = []\n",
    "    for pair in x[1]:\n",
    "        l.append((pair[0], (file, pair[1])))\n",
    "    return l\n",
    "\n",
    "\n",
    "path = '/user/hadoop/tfidf/input/*.txt'\n",
    "corpus = sc.wholeTextFiles(path)\n",
    "\n",
    "# compute number of documents in the corpus\n",
    "num_docs = corpus.count()\n",
    "# compute tf\n",
    "tf = corpus.map(lambda x: (x[0].split('/')[-1],\n",
    "                           word_freq(x[1]))).flatMap(word_to_key)\n",
    "# compute idf\n",
    "idf = tf.map(lambda x: (x[0], 1)).reduceByKey(\n",
    "    lambda x, y: x+y).map(lambda x: (x[0], math.log(num_docs/x[1])))\n",
    "# compute tfidf\n",
    "tfidf = tf.join(idf).map(lambda x: (\n",
    "    x[0], (x[1][0][0], x[1][0][1]*x[1][1]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark 2\n",
    "This document is good for a small number of document of any size because it parallelizes the lines of documents over different machines but does in a loop for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import math\n",
    "\n",
    "\n",
    "# The solution D = glob.glob(\"/user/hadoop/tfidf/input/*.txt\") returns D = [],\n",
    "# so we need to use the command line in order to get the list of files in the hdfs directory.\n",
    "cmd = 'hdfs dfs -ls /user/hadoop/tfidf/input/'\n",
    "files = subprocess.check_output(\n",
    "    cmd, shell=True).decode('utf-8').strip().split('\\n')\n",
    "D = [x.split(' ')[-1] for x in files]\n",
    "D = D[1:]\n",
    "\n",
    "num_docs = len(D)\n",
    "\n",
    "all_words_tf = sc.parallelize([])\n",
    "all_words_idf = sc.parallelize([])\n",
    "\n",
    "# for each document in the list keep unique words in all_words_idf and frequencies in all_words_tf\n",
    "for doc in D:\n",
    "    d = sc.textFile(doc)\n",
    "    words = d.flatMap(lambda s: s.lower().split()).map(lambda w: ''.join(\n",
    "        x for x in w if x.isalpha())) # x.isalpha() == True iff x is an alphabetic character. \n",
    "    all_words_idf = all_words_idf.union(words.distinct())\n",
    "    N = words.count()\n",
    "    occurrences = words.map(lambda x: (x, 1)).reduceByKey(\n",
    "        lambda x, y: x+y).map(lambda x: (x[0], (doc, x[1]/N)))\n",
    "    all_words_tf = all_words_tf.union(occurrences)\n",
    "\n",
    "# compute idf\n",
    "idf = all_words_idf.map(lambda x: (x, 1)).reduceByKey(\n",
    "    lambda x, y: x+y).map(lambda x: (x[0], math.log(num_docs/x[1])))\n",
    "\n",
    "# compute tfidf\n",
    "tfidf = all_words_tf.join(idf).map(lambda x: (\n",
    "    x[0], (x[1][0][0], x[1][0][1]*x[1][1]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Experimental analysis\n",
    "\n",
    "In order to compare the performance of the three different implementations, we designed two different tests considering different structures of a text collection with increasing sizes. We differentiated between a collection of documents containing many short documents; and a collection containing few long documents:\n",
    "\n",
    "+ Collections with a fixed document size: we fixed the number of words at 100 words per document, changing the number of documents in (100,200,300,400,500,1000).  \n",
    "+ Collections with a fixed number of documents: We ran our tf-idf algorithms on 100 documents containing (100,200,300,400,500,1000) words. \n",
    "\n",
    "For easy reproducability this report contains all code and scripts that have been used to produce the following analysis and can be recreated on AWS Elastic Map Reduce cluster. \n",
    "\n",
    "**Limitations**  \n",
    "In order to control the size of the documents used to test our algorithm, we decided to write a script creating documents for testing purposes. One document containing 100 words has a size of 11 KB when created by our script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Comparison of the implementations\n",
    "\n",
    "Spark 1, the one that uses wholeTextFiles() and puts many documents on machines is the best one in terms of clock time in both cases of longdoc and manydoc.\n",
    "In the terms of \"time\" Spark 2 is only better for longdoc but is bad for manydoc (as you can see in \"remarque column in google sheet\" there were some lost tasks so the tfidf was not computed.  \n",
    "The MapReduce implementation is the slowest option of all three and only better then the first Spark implementation as documents become longer then 300 Words (approx. 30 KB).  \n",
    "So overall the most reliable algorithm is Spark 1.\n",
    "\n",
    "\n",
    "<img src=\"./longdoc.PNG\">\n",
    "\n",
    "<img src=\"./Manydoc.PNG\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Practical Instructions to run tests on the cluster\n",
    "\n",
    "1. Create text documents on hadoop home directory  \n",
    "    + create the input directory on /user/hadoop local file system \n",
    "    + in /input, import the python script and execute the text_generator function to create documents needed, specify params as described below. \n",
    "    + move only the .txt to the hdfs hadoop input directory "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python -m nltk.downloader brown\n",
    "\n",
    "mkdir input\n",
    "cd input\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/text_docs/text_generator.py\n",
    "\n",
    "python -c 'from text_generator import create_docs; create_docs(number_of_words_per_doc = 200, num_doc = 10,startnr = 23)'\n",
    "\n",
    "cd\n",
    "\n",
    "hdfs dfs -mkdir /user/hadoop/tfidf/input\n",
    "hdfs dfs -put input/*.txt /user/hadoop/tfidf/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Download and run the MapReduce algorithms\n",
    " + give permission to access all documents\n",
    " + execute all jobs through bash script runjobs.sh\n",
    " + after retrieving the time information, clear input directories with clear.sh"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/mapper1.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/mapper2.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/mapper3.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/reducer1.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/reducer2.py\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/reducer3.py\n",
    "\n",
    "wget https://raw.githubusercontent.com/ZiggerZZ/DB-TF-IDF/master/runjobs.sh\n",
    "\n",
    "chmod +x *.py\n",
    "chmod +x *.sh\n",
    "\n",
    "time sh runjobs.sh\n",
    "\n",
    "sh clear.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Code Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text_generator.py\n",
    "**Requirements** Requires installing the nlkt library brown.   \n",
    "\n",
    "**Functionality:** The function create_docs takes as input parameters the number of words per document, number of documents, and start number of the document to be taken into account when naming files. The function creates the specified number of documents at the specified length by taking samples of the brown corpus. It randomly samples lines of twenty words at the same time.\n",
    "\n",
    "The Brown University Standard Corpus of Present-Day American English (or just Brown Corpus) was compiled in the 1960s as a general corpus. It contains 500 samples of English-language text, totaling roughly one million words, compiled from works published in the United States in 1961. It is a suitable library to sample from in order to test our algorithm as it represents how language is used in reality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "from nltk.corpus import brown\n",
    "import random\n",
    "corpus_length = len(brown.words())\n",
    "hardcopy = brown.words()\n",
    "\n",
    "\n",
    "def create_docs(number_of_words_per_doc=200, num_doc=10, startnr=0):\n",
    "    # control number of words per doc\n",
    "    # and number of documents\n",
    "    # we fix the line length at 20\n",
    "    line_length = 20\n",
    "    number_of_lines = int(number_of_words_per_doc / line_length)\n",
    "\n",
    "    for i in range(0, num_doc):\n",
    "        # create new file with writing + permission\n",
    "        new_file = open(\"textdoc\"+str(number_of_words_per_doc) +\n",
    "                        \"words\" + str(i+startnr)+\".txt\", \"w+\")\n",
    "        for line in range(0, number_of_lines):\n",
    "            words = list(map(\n",
    "                lambda x: hardcopy[x:x+line_length], random.sample(range(corpus_length), line_length)))\n",
    "            sentences = list(\n",
    "                map(lambda x: ' '.join(word for word in x), words))\n",
    "            text = ''.join(map(str, sentences))\n",
    "            new_file.write(text + \"\\n\")\n",
    "        new_file.close()\n",
    "        if (i % 10 == 0):\n",
    "            print(\"You created \"+str(i)+\" files! \"+str(num_doc-i)+\" left\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
